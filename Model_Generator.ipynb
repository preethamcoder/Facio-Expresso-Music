{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea1c7edc-d204-4c30-8227-08127c8794b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.18.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Collecting torch==2.3.1 (from torchvision)\n",
      "  Downloading torch-2.3.1-cp311-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.3.1->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.3.1->torchvision) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.3.1->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.3.1->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.3.1->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.3.1->torchvision) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch==2.3.1->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch==2.3.1->torchvision) (1.3.0)\n",
      "Downloading torchvision-0.18.1-cp311-cp311-macosx_11_0_arm64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.3.1-cp311-none-macosx_11_0_arm64.whl (61.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.3.0\n",
      "    Uninstalling torch-2.3.0:\n",
      "      Successfully uninstalled torch-2.3.0\n",
      "Successfully installed torch-2.3.1 torchvision-0.18.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d4dd23d-e6d5-407d-b649-572ed1d816ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c85a6bb0-1cbd-42fb-99a7-e43646d63ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tf.config.list_physical_devices())\n",
    "# tf.config.set_visible_devices(tf.config.list_physical_devices('CUDA'), 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38439c12-d36e-4a8e-8e9a-9ac888412715",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = ImageDataGenerator(rescale=0.255, validation_split=0.15)\n",
    "test_dataloader = ImageDataGenerator(rescale=0.255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92513424-4ebb-4758-8f7e-96aba6521ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24407 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_dataloader.flow_from_directory('train', target_size=(48,48), color_mode='grayscale', batch_size=200, class_mode='categorical', subset='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cddaf010-6239-4ca1-a277-85367b3f5f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4303 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = train_dataloader.flow_from_directory('train', target_size=(48,48), color_mode='grayscale', batch_size=64, class_mode='categorical', subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4880fb2c-cd38-4f52-9d73-ab1236a26be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_dataloader.flow_from_directory('test', target_size=(48,48), color_mode='grayscale', batch_size=64, class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a317a53-598f-4332-982c-3eaeee9a2f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Here, we are defining a CNN\n",
    "# It will have 3 convolutional layers and 1 fully conneted layer\n",
    "model = Sequential()\n",
    "\n",
    "# Conv 1\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Conv 2\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='tanh'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Conv 3\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21509cb7-c888-4d09-9374-6fbf0e886f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204d6b38-b3f4-467d-81ad-a8d0483883f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 310ms/step - accuracy: 0.2171 - loss: 2.2434 - val_accuracy: 0.2515 - val_loss: 1.8372\n",
      "Epoch 2/50\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 267ms/step - accuracy: 0.2519 - loss: 1.8216 - val_accuracy: 0.2515 - val_loss: 1.8341\n",
      "Epoch 3/50\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 313ms/step - accuracy: 0.2534 - loss: 1.8137 - val_accuracy: 0.2515 - val_loss: 1.8161\n",
      "Epoch 4/50\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 292ms/step - accuracy: 0.2476 - loss: 1.8244 - val_accuracy: 0.2517 - val_loss: 1.8123\n",
      "Epoch 5/50\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 276ms/step - accuracy: 0.2469 - loss: 1.8219 - val_accuracy: 0.2517 - val_loss: 1.8162\n",
      "Epoch 6/50\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 313ms/step - accuracy: 0.2547 - loss: 1.8200 - val_accuracy: 0.2515 - val_loss: 1.8118\n",
      "Epoch 7/50\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 292ms/step - accuracy: 0.2540 - loss: 1.8132 - val_accuracy: 0.2515 - val_loss: 1.8143\n",
      "Epoch 8/50\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 390ms/step - accuracy: 0.2503 - loss: 1.8151 - val_accuracy: 0.2512 - val_loss: 1.8115\n",
      "Epoch 9/50\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 341ms/step - accuracy: 0.2535 - loss: 1.8094 - val_accuracy: 0.2549 - val_loss: 1.8013\n",
      "Epoch 10/50\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 300ms/step - accuracy: 0.2639 - loss: 1.7917 - val_accuracy: 0.3298 - val_loss: 1.7026\n",
      "Epoch 11/50\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 289ms/step - accuracy: 0.3032 - loss: 1.7217 - val_accuracy: 0.3465 - val_loss: 1.6639\n",
      "Epoch 12/50\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 308ms/step - accuracy: 0.3321 - loss: 1.6732 - val_accuracy: 0.3758 - val_loss: 1.6031\n",
      "Epoch 13/50\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 280ms/step - accuracy: 0.3608 - loss: 1.6282 - val_accuracy: 0.3969 - val_loss: 1.5504\n",
      "Epoch 14/50\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 284ms/step - accuracy: 0.3736 - loss: 1.6028 - val_accuracy: 0.4139 - val_loss: 1.5045\n",
      "Epoch 15/50\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 296ms/step - accuracy: 0.3892 - loss: 1.5590 - val_accuracy: 0.4269 - val_loss: 1.4851\n",
      "Epoch 16/50\n",
      "\u001b[1m 93/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m8s\u001b[0m 292ms/step - accuracy: 0.4045 - loss: 1.5323"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    verbose=1\n",
    ")\n",
    "score = model.evaluate(test_generator, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]*100}')\n",
    "\n",
    "# model.save('emotion_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
